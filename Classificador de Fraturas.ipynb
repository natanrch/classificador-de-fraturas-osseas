{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c01ec7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulando imagem .gif: GII-2-137-g001.gif\n",
      "Pulando imagem .gif: ijfa-1-001-001.gif\n",
      "Pulando imagem .gif: rg.2018180064.fig1.gif\n",
      "Pulando imagem .gif: rg.2018180064.fig2.gif\n",
      "Pulando imagem .gif: rg.2020190085.fig2a.gif\n",
      "Pulando imagem .gif: rg.2015140156.fig6a.gif\n",
      "Pulando imagem .gif: rg.2020190154.fig17a.gif\n",
      "Pulando imagem .gif: radiol.2017162100.fig5a.gif\n",
      "Pulando imagem .gif: radiol.2017162100.fig5b.gif\n",
      "Pulando imagem .gif: traum1c.gif\n",
      "Pulando imagem .gif: CCASE_44-FF1.gif\n",
      "Pulando imagem .gif: CCASE_44-FF4.gif\n",
      "Pulando imagem .gif: A214966_1_En_4_Fig3b_HTML.gif\n",
      "Pulando imagem .gif: buckle-fx-distal-radius.gif\n",
      "Pulando imagem .gif: greenstick__distal__fibula..gif\n",
      "Pulando imagem .gif: rg.342135073.fig4.gif\n",
      "Pulando imagem .gif: 13244_2014_371_Fig1_HTML.gif\n",
      "Imagem inválida: ./dados\\Hairline Fracture\\bjr-85-1016-1148-f01.jpeg\n",
      "Pulando imagem .gif: rg.2018180073.fig10a.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig4b.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig4c.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig4d.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig6a.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig9.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig4b.gif\n",
      "Pulando imagem .gif: rg.342135073.fig4.gif\n",
      "Pulando imagem .gif: 12245_2015_75_Fig5_HTML.gif\n",
      "Pulando imagem .gif: rg.2016150216.fig27b.gif\n",
      "Pulando imagem .gif: Xray20-20Lat20-20Smith20Fx_moved.gif\n",
      "Imagem inválida: ./dados\\Longitudinal fracture\\bjr-85-1016-1148-f01.jpeg\n",
      "Pulando imagem .gif: C4-FF2-6.gif\n",
      "Pulando imagem .gif: fracture-complete.gif\n",
      "Pulando imagem .gif: radiol.16142305.fig3.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig10a.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig4b.gif\n",
      "Pulando imagem .gif: rg.2020190085.fig9.gif\n",
      "Pulando imagem .gif: Foot-Series.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig10a.gif\n",
      "Pulando imagem .gif: rg.2018180073.fig4c.gif\n",
      "Pulando imagem .gif: figure1.gif\n",
      "Pulando imagem .gif: figure2.gif\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "diretorio_base = './'\n",
    "\n",
    "tipos_de_fraturas = os.listdir(os.path.join(diretorio_base, 'dados'))\n",
    "\n",
    "largura_desejada = 224\n",
    "altura_desejada = 224\n",
    "\n",
    "for tipo_de_fratura in tipos_de_fraturas:\n",
    "\n",
    "    diretorio_fratura = os.path.join(diretorio_base, 'dados', tipo_de_fratura)\n",
    "\n",
    "    diretorio_saida = os.path.join(diretorio_base, 'dados_preprocessados', tipo_de_fratura)\n",
    "    os.makedirs(diretorio_saida, exist_ok=True)\n",
    "    \n",
    "    imagens = os.listdir(diretorio_fratura)\n",
    "    \n",
    "    for imagem_nome in imagens:\n",
    "        if not imagem_nome.endswith(\".gif\"):\n",
    "            imagem_path = os.path.join(diretorio_fratura, imagem_nome)\n",
    "            try:\n",
    "                imagem = cv2.imread(imagem_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if imagem is not None and imagem.shape[0] > 0 and imagem.shape[1] > 0:\n",
    "                    imagem_redimensionada = cv2.resize(imagem, (largura_desejada, altura_desejada))\n",
    "\n",
    "                    imagem_normalizada = imagem_redimensionada / 255.0\n",
    "                    \n",
    "                    imagem_saida_path = os.path.join(diretorio_saida, imagem_nome)\n",
    "                    cv2.imwrite(imagem_saida_path, (imagem_normalizada * 255).astype(np.uint8))\n",
    "                else:\n",
    "                    print(f\"Imagem inválida: {imagem_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar imagem {imagem_nome}: {str(e)}\")\n",
    "        else:\n",
    "            # Pule imagens .gif\n",
    "            print(f\"Pulando imagem .gif: {imagem_nome}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ef7414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1341 images belonging to 12 classes.\n",
      "Found 329 images belonging to 12 classes.\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 98s 2s/step - loss: 2.5511 - accuracy: 0.1051 - val_loss: 2.4513 - val_accuracy: 0.1246\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 95s 2s/step - loss: 2.3786 - accuracy: 0.1879 - val_loss: 2.4030 - val_accuracy: 0.2036\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.2159 - accuracy: 0.2782 - val_loss: 2.3475 - val_accuracy: 0.1976\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 96s 2s/step - loss: 1.9503 - accuracy: 0.3818 - val_loss: 2.2830 - val_accuracy: 0.2553\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 97s 2s/step - loss: 1.6718 - accuracy: 0.4787 - val_loss: 2.2288 - val_accuracy: 0.3435\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 97s 2s/step - loss: 1.4115 - accuracy: 0.5690 - val_loss: 2.2088 - val_accuracy: 0.3495\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 98s 2s/step - loss: 1.1836 - accuracy: 0.6525 - val_loss: 2.1885 - val_accuracy: 0.3860\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 96s 2s/step - loss: 1.0270 - accuracy: 0.6831 - val_loss: 2.2580 - val_accuracy: 0.4195\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 98s 2s/step - loss: 0.9111 - accuracy: 0.7248 - val_loss: 2.2303 - val_accuracy: 0.4225\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 98s 2s/step - loss: 0.7899 - accuracy: 0.7696 - val_loss: 2.3186 - val_accuracy: 0.4407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "caminho_base = './dados_preprocessados'\n",
    "\n",
    "batch_size = 32\n",
    "image_shape = (224, 224, 1)  # Agora estamos usando apenas 1 canal (tons de cinza)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  # 20% dos dados para validação\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',  \n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=image_shape, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10 \n",
    "model.fit(train_generator, epochs=epochs, validation_data=valid_generator)\n",
    "\n",
    "model.save('modelo_fraturas_tons_de_cinza.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e3cbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1082 images belonging to 12 classes.\n",
      "Found 264 images belonging to 12 classes.\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 73s 2s/step - loss: 2.5054 - accuracy: 0.1128 - val_loss: 2.4280 - val_accuracy: 0.1402\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 70s 2s/step - loss: 2.3771 - accuracy: 0.1848 - val_loss: 2.3529 - val_accuracy: 0.1742\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 74s 2s/step - loss: 2.2255 - accuracy: 0.2468 - val_loss: 2.2746 - val_accuracy: 0.3144\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 76s 2s/step - loss: 2.0137 - accuracy: 0.3577 - val_loss: 2.1737 - val_accuracy: 0.3598\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 73s 2s/step - loss: 1.7815 - accuracy: 0.4436 - val_loss: 2.1768 - val_accuracy: 0.3636\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 72s 2s/step - loss: 1.5358 - accuracy: 0.5379 - val_loss: 2.0367 - val_accuracy: 0.4205\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.3352 - accuracy: 0.5952 - val_loss: 2.0325 - val_accuracy: 0.4242\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 71s 2s/step - loss: 1.1819 - accuracy: 0.6423 - val_loss: 2.0512 - val_accuracy: 0.4659\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.0134 - accuracy: 0.6922 - val_loss: 2.0407 - val_accuracy: 0.4659\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 71s 2s/step - loss: 0.9421 - accuracy: 0.7181 - val_loss: 2.0167 - val_accuracy: 0.4659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "caminho_base = './dados_preprocessados_ajustes_manuais'\n",
    "\n",
    "batch_size = 32\n",
    "image_shape = (224, 224, 1) \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2 \n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale', \n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=image_shape, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10 \n",
    "model.fit(train_generator, epochs=epochs, validation_data=valid_generator)\n",
    "\n",
    "model.save('modelo_fraturas_apos_ajustes_na_base.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309258e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "diretorio_base = './'\n",
    "\n",
    "tipos_de_fraturas = os.listdir(os.path.join(diretorio_base, 'dados'))\n",
    "\n",
    "largura_desejada = 224\n",
    "altura_desejada = 224\n",
    "\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "for tipo_de_fratura in tipos_de_fraturas:\n",
    "    diretorio_fratura = os.path.join(diretorio_base, 'dados_preprocessados_ajustes_manuais', tipo_de_fratura)\n",
    "    \n",
    "    diretorio_saida = os.path.join(diretorio_base, 'dados_preprocessados_aug', tipo_de_fratura)\n",
    "    os.makedirs(diretorio_saida, exist_ok=True)\n",
    "    \n",
    "    imagens = os.listdir(diretorio_fratura)\n",
    "    \n",
    "    for imagem_nome in imagens:\n",
    "\n",
    "        if not imagem_nome.endswith(\".gif\"):\n",
    "            imagem_path = os.path.join(diretorio_fratura, imagem_nome)\n",
    "\n",
    "            try:\n",
    "\n",
    "                imagem = cv2.imread(imagem_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if imagem is not None and imagem.shape[0] > 0 and imagem.shape[1] > 0:\n",
    "                    imagem_redimensionada = cv2.resize(imagem, (largura_desejada, altura_desejada))\n",
    "                    imagem_redimensionada = np.expand_dims(imagem_redimensionada, axis=-1)  # Adicione a dimensão de canais (1)\n",
    "\n",
    "                    batch_images = np.array([imagem_redimensionada])\n",
    "\n",
    "                    augmented_images = []\n",
    "                    i = 0\n",
    "                    for batch in data_generator.flow(batch_images, batch_size=1):\n",
    "                        augmented_image = batch[0]\n",
    "                        augmented_images.append(augmented_image)\n",
    "                        i += 1\n",
    "                        if i >= 5:  \n",
    "                            break\n",
    "\n",
    "                    for j, augmented_image in enumerate(augmented_images):\n",
    "                        imagem_saida_path = os.path.join(diretorio_saida, f\"{imagem_nome.split('.')[0]}_{j}.jpg\")\n",
    "                        cv2.imwrite(imagem_saida_path, augmented_image)\n",
    "                else:\n",
    "                    print(f\"Imagem inválida: {imagem_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar imagem {imagem_nome}: {str(e)}\")\n",
    "        else:\n",
    "            # Pule imagens .gif\n",
    "            print(f\"Pulando imagem .gif: {imagem_nome}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02c7cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5360 images belonging to 12 classes.\n",
      "Found 1340 images belonging to 12 classes.\n",
      "Epoch 1/10\n",
      "168/168 [==============================] - 353s 2s/step - loss: 2.4603 - accuracy: 0.1252 - val_loss: 2.4152 - val_accuracy: 0.1358\n",
      "Epoch 2/10\n",
      "168/168 [==============================] - 351s 2s/step - loss: 2.3738 - accuracy: 0.1679 - val_loss: 2.3878 - val_accuracy: 0.1552\n",
      "Epoch 3/10\n",
      "168/168 [==============================] - 349s 2s/step - loss: 2.2864 - accuracy: 0.2069 - val_loss: 2.3882 - val_accuracy: 0.1739\n",
      "Epoch 4/10\n",
      "168/168 [==============================] - 350s 2s/step - loss: 2.1076 - accuracy: 0.2826 - val_loss: 2.4224 - val_accuracy: 0.1828\n",
      "Epoch 5/10\n",
      "168/168 [==============================] - 349s 2s/step - loss: 1.8942 - accuracy: 0.3640 - val_loss: 2.4518 - val_accuracy: 0.2037\n",
      "Epoch 6/10\n",
      "168/168 [==============================] - 348s 2s/step - loss: 1.6428 - accuracy: 0.4631 - val_loss: 2.5591 - val_accuracy: 0.2060\n",
      "Epoch 7/10\n",
      "168/168 [==============================] - 348s 2s/step - loss: 1.3797 - accuracy: 0.5479 - val_loss: 2.6483 - val_accuracy: 0.2082\n",
      "Epoch 8/10\n",
      "168/168 [==============================] - 348s 2s/step - loss: 1.1500 - accuracy: 0.6280 - val_loss: 2.8792 - val_accuracy: 0.2067\n",
      "Epoch 9/10\n",
      "168/168 [==============================] - 349s 2s/step - loss: 0.9334 - accuracy: 0.7034 - val_loss: 3.0338 - val_accuracy: 0.1985\n",
      "Epoch 10/10\n",
      "168/168 [==============================] - 348s 2s/step - loss: 0.7532 - accuracy: 0.7709 - val_loss: 3.2413 - val_accuracy: 0.1910\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "caminho_base = './dados_preprocessados_aug'\n",
    "\n",
    "batch_size = 32\n",
    "image_shape = (224, 224, 1)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  \n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale', \n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=image_shape, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10  \n",
    "model.fit(train_generator, epochs=epochs, validation_data=valid_generator)\n",
    "\n",
    "model.save('modelo_fraturas_apos_ajustes_na_base_com_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7722c836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5360 images belonging to 12 classes.\n",
      "Found 1340 images belonging to 12 classes.\n",
      "Epoch 1/10\n",
      "168/168 [==============================] - 782s 5s/step - loss: 2.4429 - accuracy: 0.1552 - val_loss: 2.3223 - val_accuracy: 0.2246\n",
      "Epoch 2/10\n",
      "168/168 [==============================] - 782s 5s/step - loss: 2.2469 - accuracy: 0.2405 - val_loss: 2.2611 - val_accuracy: 0.2716\n",
      "Epoch 3/10\n",
      "168/168 [==============================] - 785s 5s/step - loss: 2.1078 - accuracy: 0.2912 - val_loss: 2.1716 - val_accuracy: 0.3172\n",
      "Epoch 4/10\n",
      "168/168 [==============================] - 789s 5s/step - loss: 1.9880 - accuracy: 0.3356 - val_loss: 2.1039 - val_accuracy: 0.3388\n",
      "Epoch 5/10\n",
      "168/168 [==============================] - 787s 5s/step - loss: 1.8914 - accuracy: 0.3823 - val_loss: 2.0786 - val_accuracy: 0.3478\n",
      "Epoch 6/10\n",
      "168/168 [==============================] - 834s 5s/step - loss: 1.7944 - accuracy: 0.4131 - val_loss: 2.0519 - val_accuracy: 0.3500\n",
      "Epoch 7/10\n",
      "168/168 [==============================] - 792s 5s/step - loss: 1.7240 - accuracy: 0.4440 - val_loss: 2.0326 - val_accuracy: 0.3545\n",
      "Epoch 8/10\n",
      "168/168 [==============================] - 793s 5s/step - loss: 1.6330 - accuracy: 0.4713 - val_loss: 2.0267 - val_accuracy: 0.3530\n",
      "Epoch 9/10\n",
      "168/168 [==============================] - 797s 5s/step - loss: 1.5637 - accuracy: 0.4950 - val_loss: 2.0130 - val_accuracy: 0.3687\n",
      "Epoch 10/10\n",
      "168/168 [==============================] - 792s 5s/step - loss: 1.4994 - accuracy: 0.5272 - val_loss: 2.0056 - val_accuracy: 0.3612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "caminho_base = './dados_preprocessados_aug'\n",
    "\n",
    "batch_size = 32\n",
    "image_shape = (224, 224, 3)  \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  \n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',  \n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=image_shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10 \n",
    "model.fit(train_generator, epochs=epochs, validation_data=valid_generator)\n",
    "\n",
    "model.save('modelo_fraturas_transfer_learning.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6d0362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1082 images belonging to 12 classes.\n",
      "Found 264 images belonging to 12 classes.\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 162s 5s/step - loss: 2.5545 - accuracy: 0.1322 - val_loss: 2.3270 - val_accuracy: 0.1439\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 163s 5s/step - loss: 2.2687 - accuracy: 0.2283 - val_loss: 2.2004 - val_accuracy: 0.2727\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 162s 5s/step - loss: 2.0787 - accuracy: 0.3133 - val_loss: 2.1155 - val_accuracy: 0.3712\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 161s 5s/step - loss: 1.9233 - accuracy: 0.3697 - val_loss: 2.0073 - val_accuracy: 0.3788\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 163s 5s/step - loss: 1.7412 - accuracy: 0.4640 - val_loss: 1.9439 - val_accuracy: 0.4167\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 161s 5s/step - loss: 1.6124 - accuracy: 0.5092 - val_loss: 1.8764 - val_accuracy: 0.4280\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 169s 5s/step - loss: 1.5057 - accuracy: 0.5490 - val_loss: 1.8332 - val_accuracy: 0.4848\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 160s 5s/step - loss: 1.4108 - accuracy: 0.5638 - val_loss: 1.7910 - val_accuracy: 0.4697\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 162s 5s/step - loss: 1.2910 - accuracy: 0.6257 - val_loss: 1.7295 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 164s 5s/step - loss: 1.2344 - accuracy: 0.6460 - val_loss: 1.6834 - val_accuracy: 0.5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "caminho_base = './dados_preprocessados_ajustes_manuais'\n",
    "\n",
    "batch_size = 32\n",
    "image_shape = (224, 224, 3)  \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  \n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',  \n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=image_shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))\n",
    "\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10  # Defina o número desejado de épocas\n",
    "model.fit(train_generator, epochs=epochs, validation_data=valid_generator)\n",
    "\n",
    "\n",
    "model.save('modelo_fraturas_transfer_learning.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e06038",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (428475996.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 42\u001b[1;36m\u001b[0m\n\u001b[1;33m    model.add(MaxPooling2D((2, 2))\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb6dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1082 images belonging to 12 classes.\n",
      "Found 264 images belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - 164s 5s/step - loss: 2.5548 - accuracy: 0.1312 - val_loss: 2.2815 - val_accuracy: 0.2197\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 162s 5s/step - loss: 2.1878 - accuracy: 0.2542 - val_loss: 2.1210 - val_accuracy: 0.3409\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 165s 5s/step - loss: 1.9741 - accuracy: 0.3632 - val_loss: 2.0094 - val_accuracy: 0.3598\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 164s 5s/step - loss: 1.7859 - accuracy: 0.4214 - val_loss: 1.9255 - val_accuracy: 0.4356\n",
      "Epoch 5/20\n",
      "16/34 [=============>................] - ETA: 26:09:29 - loss: 1.6269 - accuracy: 0.4881"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Treine o modelo\u001b[39;00m\n\u001b[0;32m     57\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# Defina o número desejado de épocas\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_generator, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_data\u001b[38;5;241m=\u001b[39mvalid_generator)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Salve o modelo treinado\u001b[39;00m\n\u001b[0;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelo_fraturas_transfer_learning.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "caminho_base = './dados_preprocessados_ajustes_manuais'\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "image_shape = (224, 224, 3)  # Alterado para 1 canal (tons de cinza)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  \n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',  \n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    caminho_base,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=image_shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)  \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 20  \n",
    "model.fit(train_generator, epochs=epochs, validation_data=valid_generator)\n",
    "\n",
    "model.save('modelo_fraturas_transfer_learning.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2cbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
